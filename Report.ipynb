{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "In this environment, I use [Twin Delayed Deep Deterministic Policy](https://arxiv.org/abs/1802.09477) which so called TD3 to train the agents. There are two agents, they share a replay buffer and critic networks and the actor network is independence for each agent. While training, agents perform their action according to their state and adding their record to the same replay buffer. \n",
    "\n",
    "TD3 is based on [Deep Deterministic Policy Gradient(DDPG)](https://arxiv.org/abs/1509.02971) and have several modifications on it. The main ideas are in the following list.\n",
    "\n",
    "* **Clipped Double Q-Learning for Actor-Critic**\n",
    "\n",
    "    It uses the concept of Double Q-Learning to alleviate the overestimation bias problem in Actor-Critic. To be specific, it\n",
    "    have two pairs of critic networks(2 locals and 2 targets). While calculating target Q values, it uses both critic networks \n",
    "    to compute and choose the smaller outcome as target Q value to update the network. By doing this to reduce the influence of \n",
    "    the overestimation problem.\n",
    "    \n",
    "\n",
    "* **Target Networks and Delayed Policy Updates**\n",
    "\n",
    "    Just like the DDPG, TD3 also use target networks and soft update method to stabilize the training procedure. Beside these\n",
    "    methods, it also uses delayed updates which simply only update target networks and actor networks every several timesteps. \n",
    "    This way can result in higher quality policy updates thus improve the performance of the agent.\n",
    "\n",
    "\n",
    "* **Target Policy Smoothing Regularization**\n",
    "\n",
    "    Intuitively, similar actions should have similar value. So, in order to implement this idea. TD3 adding a small amount \n",
    "    of random noise to the target policy. That is to say, fitting the value of a small area around the target action would have \n",
    "    the beneﬁt of smoothing the value estimate by similar state-action value estimates.\n",
    "    \n",
    "This is the pseudo code from the paper\n",
    "\n",
    "![pseudo code](img/td3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "**The actor model architecture**\n",
    "\n",
    "```\n",
    "Actor(\n",
    "  (fc1): Linear(in_features=24, out_features=256, bias=True)\n",
    "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
    "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
    ")\n",
    "```\n",
    "\n",
    "**The critic model architecture**\n",
    "\n",
    "```\n",
    "Critic(\n",
    "  (fc1): Linear(in_features=24, out_features=256, bias=True)\n",
    "  (fc2): Linear(in_features=258, out_features=128, bias=True)\n",
    "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
    ")\n",
    "```\n",
    "\n",
    "Note that the critic model architecture is different from the TD3 author's implementation. The offical TD3 critic model implementation concatenates the state and action before the first dense layer, which the Udacity's DDPG implementation concatenates them in the second layer. I found that the agent can not learn anything while using the TD3 critic implementation. At the end, I choose to use DDPG critic model architecture.\n",
    "\n",
    "**The code comparison**\n",
    "\n",
    "```\n",
    "class DDPGCritic(nn.Module):\n",
    "    \"\"\"DDPG Critic (Value) Model.\"\"\"\n",
    "    def __init__(self, state_size, action_size, fc1_units=256, fc2_units=128):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.leaky_relu(self.fc1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class TD3Critic(nn.Module):\n",
    "    \"\"\"TD3 Critic (Value) Model.\"\"\"\n",
    "    def __init__(self, state_size, action_size, fc1_units=256, fc2_units=128):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size + action_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        state_action = torch.cat([state, action], dim=1)\n",
    "        xs = F.relu(self.fc1(state_action))\n",
    "        x = F.relu(self.fc2(xs))\n",
    "        return self.fc3(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 256        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-2              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "LEARN_EVERY = 4        # how often to learn from the experience\n",
    "UPDATE_EVERY = 2        # how often to update the target network\n",
    "random_seed = 0\n",
    "```\n",
    "\n",
    "**TD3 noise parameters**\n",
    "\n",
    "```\n",
    "noise = 0.2             # the range to generate random noise while learning\n",
    "noise_std = 0.3         # the range to generate random noise while performing action\n",
    "noise_clip = 0.5        # to clip random noise into this range\n",
    "```\n",
    "\n",
    "**Training noise parameters**\n",
    "```\n",
    "# apply to the generated random noise while performing action for exploration at the beginning of every episode.\n",
    "noise_degree = 2.0    \n",
    "\n",
    "# reduce noise degree every timestep.\n",
    "noise_decay = 0.999     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result \n",
    "\n",
    "The environment is considered solved, when the average (over 100 episodes) of those scores is at least +0.5. The left plot is the maximum score between two agents in each episode, the right plot is the average scores of two agents in each episode. We can see that the score is over 0.5 at around 470 episode.\n",
    "\n",
    "![Plot](img/plot.png)\n",
    "\n",
    "![Agent](img/agent.gif)\n",
    "\n",
    "## Ideas for Future Work\n",
    "\n",
    "I have only tried DDPG and TD3 in this environment, so maybe other algorithms like PPO or D4PG would work better in this environment. And also continuing tune the hyper parameters could improve the performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
